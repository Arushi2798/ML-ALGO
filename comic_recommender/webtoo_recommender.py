# -*- coding: utf-8 -*-
"""webtoo_recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-nNl68FJkHltrvdDpzWHyEnqy1ueeqrR
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import numpy as np

base_url = "https://www.webtoons.com/en/dailySchedule"
headers = {"User-Agent": "Mozilla/5.0"}

response = requests.get(base_url, headers=headers)
soup = BeautifulSoup(response.content, "html.parser")

import os

# This will delete the entire folder and its contents recursively.
# import shutil

# shutil.rmtree('/content/webtoon_images')

# Create directory to save images
os.makedirs("webtoon_images", exist_ok=True)

# Final data lists
comics,titles, authors, ratings, descriptions,episodes,genres,votes,completed,image_url = [],[],[], [], [], [],[],[],[],[]
visited_comics = set()

# Step 1: Get all comic URLs from the daily page
# for i, comic_card in enumerate(soup.select(".daily_card_item")):
#     if i >= 3:
#         break

for i, comic_card in enumerate(soup.select(".daily_card_item")):
   # Skip if already visited
    if comic_card in visited_comics:
        continue

    visited_comics.add(comic_card)

    title = comic_card.select_one(".subj").get_text(strip=True)

    # Image from daily card
    img_tag = comic_card.select_one("img")
    img_url = img_tag.get("src") if img_tag else None

    if img_url and img_url.startswith("http"):
      try:
        response = requests.get(img_url)
        safe_title = title.replace(" ", "_").replace("/", "_")
        img_filename = f"webtoon_images/{safe_title}.jpg"
        with open(img_filename, "wb") as f:
          f.write(response.content)
        print(f"‚úÖ Downloaded image: {img_filename}")
      except:
        img_filename = "N/A"
        print(f"‚ùå Failed to download image for {title}")
    else:
      img_filename = "N/A"

    genre = comic_card.select_one(".genre").get_text(strip=True)
    grade_tag = comic_card.select_one("p.grade_area em.grade_num")
    grade_text = grade_tag.get_text(strip=True) if grade_tag else "N/A"
    comic_url = comic_card.get("href")

    print(f"[{i+1}] Fetching: {title}")

    # Request the comic detail page
    comic_res = requests.get(comic_url, headers=headers,timeout=10)
    comic_soup = BeautifulSoup(comic_res.content, "html.parser")

    # Extract Author

    author = comic_soup.select_one(".author_area").get_text(strip=True)

    # # Extract Rating
    rating_tag = comic_soup.select_one("em#_starScoreAverage.cnt")
    rating = rating_tag.get_text(strip=True) if rating_tag else "N/A"

    # Extract episode numbers
    episode = comic_soup.select_one("._episodeItem  .tx").get_text(strip=True)

    #extract info about completion status of comic
    update = comic_soup.select_one("p.day_info").get_text(strip=True)

    # # Extract Description
    desc_tag = comic_soup.select_one(".detail_body .summary")
    description = desc_tag.get_text(strip=True) if desc_tag else "No description"

    # Store data
    image_url.append(img_url)
    comics.append(comic_url)
    completed.append(update)
    titles.append(title)
    genres.append(genre)
    authors.append(author)
    votes.append(grade_text)
    ratings.append(rating)
    episodes.append(episode)
    descriptions.append(description)


    # Be polite with delay (1 second)
    # time.sleep(1)

# Step 2: Save to DataFrame
df = pd.DataFrame({
    "Title": titles,
    "genre": genres,
    "Description": descriptions,
    "Author": authors,
    "Rating": ratings,
    "votes": votes,
    "Total_Episodes": episodes,
    'completion_status':completed,
    "links": comics ,
    "Img_url":image_url
})

# df.to_csv("webtoon_detailed_comics.csv", index=False)
print(f"‚úÖ Scraped {len(df)} manga entries with full detail!")

print(comic_card.prettify())

print(comic_soup.prettify())

df.sample(3)

df.describe()

df.info()

# Remove the exact phrase "author info" (case insensitive), then clean up extra spaces
df["Author"] = df["Author"].str.replace(r"author info", "", regex=True)
df["Author"] = df["Author"].str.replace(r"\s+", " ", regex=True).str.strip()

df["Total_Episodes"] = df["Total_Episodes"].str.replace("#", "").astype(int)

def parse_votes(v):
    if "M" in v:
        return float(v.replace("M", "")) * 1_000_000
    elif "K" in v:
        return float(v.replace("K", "")) * 1_000
    elif "B" in v:
        return float(v.replace("B", "")) * 1_000_000_000
    else:
        return float(v.replace(",", ""))

df["votes"] = df["votes"].apply(parse_votes)

df['Total_Episodes'] = df['Total_Episodes'].astype(int)
df['Rating'] = df['Rating'].astype(float)
df['votes'] = df['votes'].astype(int)

df.info()

df.shape

df.size

df.isnull().sum()

df.describe()

df.to_csv("webtoon_detailed_comics.csv", index=False)
print(f"‚úÖ Scraped {len(df)} comics and saved to 'webtoon_detailed_comics.csv'")

df.sample(5)

df.duplicated().sum()

duplicates = df[df.duplicated()]
print(duplicates)

df = df.drop_duplicates()

df.duplicated().sum()

# Fill any missing with 0
df['Rating'] = df['Rating'].fillna(0)
df['votes'] = df['votes'].fillna(0)

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(df['genre'])

df['genre'].value_counts().plot(kind='pie',autopct='%.2f')

plt.hist(df['votes'])

sns.scatterplot(x=df['Rating'], y=df['Total_Episodes'], hue=df["genre"])

sns.barplot(x=df['genre'], y=df['Rating'])

!pip install ydata-profiling

from ydata_profiling import ProfileReport
data=df.iloc[:,0:8]
prof = ProfileReport(data, title="webtoon_profile")
prof.to_file(output_file='webtoon_output.html')

"""**Popularity Based Recommender System**


select the popular books who have at least 2517495(mean no. of votes)
"""

popular_df = df[df['votes']>= 2517495].sort_values('votes',ascending=False).head(50).reset_index().drop('index',axis=1)

popular_df.sample(5)

"""top rated books in each genre"""

top_rated_per_genre = df.loc[df.groupby('genre')['Rating'].idxmax()].reset_index()

print("Top Rated Comics by Genre:")
print(top_rated_per_genre[['genre', 'Title', 'Rating']])

def recommend_top_10_by_genre(selected_genre):
    filtered = df[df['genre'].str.lower() == selected_genre.lower()]
    top_10 = filtered.sort_values('votes', ascending=False).head(10).reset_index(drop=True)

    print(f"\nTop 10 Liked Comics in Genre: {selected_genre}")
    print(top_10[['Title', 'votes', 'Rating',"completion_status"]])

    return top_10

recommend_top_10_by_genre('Romance')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# Fill missing descriptions if any
df['Description'] = df['Description'].fillna("")

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(df['Description'])

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
indices = pd.Series(df.index, index=df['Title']).drop_duplicates()

def recommend_comics(title, num_recommendations=10):
    if title not in indices:
        return f"Comic '{title}' not found in dataset."

    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:num_recommendations+1]

    comic_indices = [i[0] for i in sim_scores]
    return df[['Title', 'Author', 'genre', 'Rating','completion_status']].iloc[comic_indices].reset_index(drop=True)

recommend_comics("The Mafia Nanny")

df.to_pickle('comics.pkl')

"""---



---

# **For future work**
"""

top_comics_each_genre = df.groupby('genre').apply(
    lambda x: x.groupby('Title').agg(
        avg_rating=('Rating', 'mean'),
        num_ratings=('Rating', 'count'),
        Author=('Author', 'first')
    ).sort_values(by='avg_rating', ascending=False).head(1)
).reset_index()

top_comics_each_genre

def top_10_comics_by_genre(genre):
    genre_comics = df[df['genre'] == genre]
    top_comics = genre_comics.groupby('Title').agg(
        avg_rating=('Rating', 'mean'),
        num_ratings=('Rating', 'count'),
        Author=('Author', 'first')
    ).sort_values(by='avg_rating', ascending=False).head(10)
    return top_comics.reset_index()

top_10_comics_by_genre("Action")

pt = df.pivot_table(index='Title', values='Rating')
pt.fillna(0, inplace=True)

from sklearn.metrics.pairwise import cosine_similarity
similarity_scores = cosine_similarity(pt)

similarity_scores

def recommend_comics(comic_title):
    if comic_title not in pt.index:
        print(f"‚ùå Comic '{comic_title}' not found in pivot table.")
        return []

    index = np.where(pt.index == comic_title)[0][0]
    similar_items = sorted(list(enumerate(similarity_scores[index])), key=lambda x: x[1], reverse=True)[1:6]

    recommendations = []
    for i in similar_items:
        comic_name = pt.index[i[0]]
        author = df[df['Title'] == comic_name]['Author'].iloc[0]
        genre = df[df['Title'] == comic_name]['genre'].iloc[0]
        recommendations.append([comic_name, author, genre])
    return recommendations

recommend_comics("Attack on Titan")

from difflib import get_close_matches

def recommend_comics(comic_title):
    # Fuzzy match the closest title
    matched_title = get_close_matches(comic_title, pt.index, n=1, cutoff=0.6)

    if not matched_title:
        print(f"‚ùå No similar title found for '{comic_title}'. Try another name.")
        return []

    matched_title = matched_title[0]  # Use the closest match
    print(f"üîç Using closest match: '{matched_title}'")

    index = np.where(pt.index == matched_title)[0][0]
    similar_items = sorted(list(enumerate(similarity_scores[index])), key=lambda x: x[1], reverse=True)[1:6]

    recommendations = []
    for i in similar_items:
        comic_name = pt.index[i[0]]
        author = df[df['Title'] == comic_name]['Author'].iloc[0]
        genre = df[df['Title'] == comic_name]['genre'].iloc[0]
        recommendations.append([comic_name, author, genre])

    return recommendations

recommend_comics("mafia nanny")

pickle.dump(pt,open('pt.pkl','wb'))
pickle.dump(similarity_scores,open('similarity.pkl','wb'))